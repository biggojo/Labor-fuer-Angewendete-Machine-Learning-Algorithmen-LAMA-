{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Regression with Neural Networks\n",
    "\n",
    "Artificial neural networks are used to solve an extensive variety of problems. In this chapter we will focus on the problem of regression, since it is the easiest to begin with. We will explore the foundations of neural networks by using them to solve exemplary regression tasks. At first, the single artificial neuron is introduced. In the next step, activation functions are made familiar through interactive tasks. Finally, backpropagation is explained at the end of the chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The Artificial Neuron in Theory\n",
    "\n",
    "\n",
    "An artificial neuron is a mathematical function that is inspired by the information processing of a biological neural cell. Each neuron accepts one or multiple values as inputs ($x_n$) and outputs one value (y). It thereby performs a simple mathematical operation. (see (1) and Fig. 1):\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "f_{\\text{neuron}}(x) = \\phi\\left(\\sum_{n=1}^m {x_n w_{n}} + b\\right)       \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;        (1)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "- **Inputs** $x_n$ are numerical values given by the data or by other neurons\n",
    "- **Weights** $w_{n}$ multiply the input values\n",
    "- A **summation $v$** of the weighted inputs is calculated\n",
    "- A **constant value** $b$ is added to the sum (so called **Bias**)\n",
    "- An **activation function $ùúô$** is applied to the sum\n",
    "- The **output** $y$ can be used as an input for another neuron or as a final output of a network\n",
    "\n",
    "The resulting output is also called the \"activation\" $y$ of the neuron. \n",
    "Even though this mechanism is very simple, a multitude of simple neurons is able to solve very complex problems.\n",
    "\n",
    "\n",
    "<img src=\"images/neural_network.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 1 - General artificial neuron\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>   \n",
    "<ul>\n",
    "<li> An artificial neuron is just an abstract concept. Even though we will create _neuron objects_ in this Jupyter Notebook for code reusability, a neuron is not bound to any specific shape or implementation. There are even approaches to realize artificial neurons with <a href=\"https://www.osapublishing.org/optica/abstract.cfm?uri=optica-6-9-1132\">pure optics</a>.\n",
    "As long as something shows a behaviour which can be described by the mathematical function (1), we can view it as a \"neuron\".\n",
    "<li> You could integrate the bias into the sum in (1) by denoting it as $w_0$ with a corresponding $x_0 = 1$.\n",
    "</li>\n",
    "\n",
    "\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 A Simple Neuron in Practice\n",
    "For the sake of explanation, we will now examine a neuron with only one single input and without any activation function. This neuron is already able to model functions with the form (2). The neuron can be visualized as seen in Figure 2.\n",
    "\n",
    "\\begin{align}\n",
    "f_{\\text{neuron}}(x) & = w * x + b \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;        (2)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/single_neuron_no_activation.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 2 - Simple artificial neuron\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neuron class will just have one input, one weight and one bias. \n",
    "- Upon initialization, it will be connected to an interactive plot\n",
    "- Its weights and biases can be changed using the set_values method.\n",
    "- Its weights and biases can be polled using the get_weights/get_bias methods.\n",
    "- When the Neuron is changed, it notifies the interactive plot to redraw its output\n",
    "- It has a compute method that computes the activation based in the weight and input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define a neuron class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='simple_neuron'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='simple_neuron'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.3 The Problem of Regression\n",
    "In the task of regression analysis, a model function has to be found that matches a given set of data points N as **accurately** as possible. A commonly used metric for the accuracy of the approximation is the **least squares approach**. The distance between each data point $(x_n, y_n)$ and the predicted value from the model $\\hat{f}(x_n)$ is calculated via the distance of the y-values of the data point with the predicted y-value from the model function (3).\n",
    "\n",
    "\\begin{align}\n",
    "d(\\hat{f}(x_n), y_n) & = \\left|\\hat{f}(x_n) - y_n\\right| \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;        (3)\n",
    "\\end{align}\n",
    "\n",
    "The distances are then squared and summed up. Since we want to compare the quality of an approximation with other approximations that might have a different amount of data points, we also divide the sum by the total number of data points (**Mean Squared Error**). This will be our **Loss** $J$ (4). Our goal is to keep this metric as low as possible, since the lower the loss, the better the approximation. Here, the terms \"loss\" and \"error\" have the same meaning. Another often used term is \"cost\".\n",
    "\n",
    "\\begin{align}\n",
    "J & = \\frac{1}{N} \\sum_{n=0}^N (\\hat{f}(x_n) - y_n)^2 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;        (4)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Note that in eq.(4) $x_0$ is not a feature like in eq.(1), but a sample of the dataset.   \n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If we have achieved an accurate regression, we can make **predictions** with it. We will train our neurons to match a given set of points and then use them to predict new points. To do so, we will give the trained neuron new x-values and it will predict y-values.\n",
    "\n",
    "\n",
    "<img src=\"images/least_squares_explanation.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 3 - Distance to model function visualized\n",
    "</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations # Used to allow referencing classes that have not yet been defined in type annotations. This will become default behaviour in Python 3.10. Until then, we have to use this line to enable that behaviour\n",
    "from typing import *\n",
    "\n",
    "\n",
    "\n",
    "class SimpleNeuron:\n",
    "    def __init__(self, plot: Interactive2DPlot):\n",
    "        self.plot = plot #I am assigned the following plot\n",
    "        self.plot.register_neuron(self) #hey plot, remember me\n",
    "        \n",
    "    def set_values(self, weight: float, bias: float):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.plot.update() #hey plot, I have changed, redraw my output\n",
    "        \n",
    "    def get_weight(self) -> float:\n",
    "        return self.weight\n",
    "    \n",
    "    def get_bias(self) -> float:\n",
    "        return self.bias\n",
    "\n",
    "    def compute(self, x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        self.activation = np.dot(self.weight, x) + self.bias\n",
    "        return self.activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function \"loss\" that performs the operation (4). It will receive a neuron object and a set of points as arguments.\n",
    "- For each point that we give it, it first separates x and y-values. \n",
    "- It hands the neuron an x-value and asks the neuron to compute a prediction for the y-value. (see $\\hat{f}(x_n)$) \n",
    "- Then it subtracts the real y-value from the predicted y-value, as in operation (3), resulting in a distance\n",
    "- It then squares up the distance and accumulates the squared distances.  \n",
    "- In the last step, it divides the sum of squared distances by the amount of compared points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(neuron: SimpleNeuron, points: Dict[str, List[float]]) -> float:\n",
    "    sum_squared_dist = 0\n",
    "\n",
    "    for point_x, point_y in zip(points[\"x\"], points[\"y\"]):  # zip merges both points[\"x\"] and points[\"y\"]\n",
    "\n",
    "        predicted_point_y = neuron.compute(point_x)\n",
    "        dist = point_y - predicted_point_y\n",
    "        squared_dist = dist ** 2\n",
    "        sum_squared_dist += squared_dist\n",
    "\n",
    "    loss = sum_squared_dist / len(points[\"y\"])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Preparing an Interactive Plot\n",
    "\n",
    "After importing the necessary libraries, we will set up an interactive plot class. It plots the output of a neuron by asking it to compute a set of x-values, which results in a set of predicted y-values that can be drawn on a plane. If the weight or bias of a neuron is changed, the neuron calls the \"redraw\" method of its plot to update it. The plot can also plot fixed points. Interactive sliders will be used to directly modify the weights and biases of neuron objects.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The plot classes are not part of the subject matter for this lab.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to import libraries and define an interactive plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.offline as plotly\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import interact, Layout, HBox, FloatSlider\n",
    "import time\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Interactive Plot monitors the activation of a neuron or a neural network\n",
    "class Interactive2DPlot:\n",
    "    def __init__(self, points: Dict[str, List[float]], ranges: Dict[str, Tuple[float, float]], width: int = 800, height: int = 400, margin: Dict[str, int] = { 't': 0, 'l': 170 }, draw_time: float = 0.05):\n",
    "        self.idle = True\n",
    "        self.points = points\n",
    "        self.x = np.arange(ranges[\"x\"][0], ranges[\"x\"][1], 0.1)\n",
    "        self.y = np.arange(ranges[\"y\"][0], ranges[\"y\"][1], 0.1)\n",
    "        self.draw_time = draw_time\n",
    "        self.layout = go.Layout(\n",
    "            xaxis=dict(title=\"Input: x\", range=ranges[\"x\"], fixedrange=True),\n",
    "            yaxis=dict(title=\"Output: y\", range=ranges[\"y\"], fixedrange=True),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            showlegend=False,\n",
    "            autosize=False,\n",
    "            margin=margin,\n",
    "        )\n",
    "        self.trace = go.Scatter(x=self.x, y=self.y)\n",
    "        self.plot_points = go.Scatter(x=points[\"x\"], y=points[\"y\"], mode=\"markers\")\n",
    "        self.data = [self.trace, self.plot_points]\n",
    "        self.plot = go.FigureWidget(self.data, self.layout)\n",
    "        # self.plot = plotly.iplot(self.data, self.layout,config={\"displayModeBar\": False})\n",
    "\n",
    "    def register_neuron(self, neuron: SimpleNeuron):\n",
    "        self.neuron = neuron\n",
    "\n",
    "    def redraw(self):\n",
    "        self.idle = False\n",
    "        time.sleep(self.draw_time)\n",
    "        self.plot.data[0].y = self.neuron.compute(self.x)\n",
    "        self.idle = True\n",
    "\n",
    "    def update(self):\n",
    "        print(\"Loss: {:0.2f}\".format(loss(self.neuron, self.points)))\n",
    "        if self.idle:\n",
    "            thread = threading.Thread(target=self.redraw)\n",
    "            thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Train the neuron\n",
    "<ul>\n",
    "<li> You are given a set of 3 points and one neuron to do a curve fit. Run the cell below.\n",
    "<li> <b>Change the weight and bias of the neuron using the sliders to minimize the loss.</b>\n",
    "    <li><b>Hint:</b> You can also change the sliders with the arrow keys on your keyboard after clicking on the slider.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='points_linreg'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ca0d1b4448440296a8aa0bea0dacd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=3.0, min=-3‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadcaf09d0784d01bd3d94ed9487a7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '4dd0e651-39dc-438a-b026-efc58371b190',\n",
       " ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "points_linreg = dict(x=[1, 2, 3], y=[1.5, 0.7, 1.2])\n",
    "ranges_linreg = dict(x=(-4, 4), y=(-4, 4))\n",
    "\n",
    "linreg_plot = Interactive2DPlot(points_linreg, ranges_linreg)\n",
    "simple_neuron = SimpleNeuron(linreg_plot)\n",
    "\n",
    "slider_layout = Layout(width=\"90%\")\n",
    "\n",
    "interact(\n",
    "    simple_neuron.set_values, \n",
    "    weight=FloatSlider(min=-3, max=3, step=0.1, value = 0, layout=slider_layout),\n",
    "    bias=FloatSlider(min=-3, max=3, step=0.1, value = 0, layout=slider_layout)\n",
    ")\n",
    "\n",
    "linreg_plot.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b> What is the optimal weight and bias combination? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>The optimal values are : weight = -0.10 and bias = 1.40 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Preparing a 3D-Plot\n",
    "We can see that searching for the lowest loss is a **parameter optimization problem**. For now, the problem can be solved manually, but if we want to use neural networks to solve more complex problems, we have to find a way to automate this process.\n",
    "\n",
    "The loss function is changed with both the specified weight and the specified bias. This relationship can be visualized three-dimensionally, which can give us further insight to construct an algorithm that solves the optimization problem. \n",
    "In this 3D-View, logarithmic scales are used to emphasize the topography. We will define a new function to compute the logarithmic loss for a set of points.\n",
    "\n",
    "The plot will be defined as follows:\n",
    "- The **X axis** represents the weights. \n",
    "- The **Y axis** represents the bias.\n",
    "- The **Z axis** (height) represents the corresponding loss value at a given weight/bias configuration. For illustration purposes, the logarithm of the MSE Loss is displayed.\n",
    "- The **black ball** represents the current weight/bias configuration. Its height represents the loss of that configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to define a 3D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mse(neuron: SimpleNeuron, points: Dict[str, List[float]]) -> np.ndarray:\n",
    "    least_squares_loss = loss(neuron, points)\n",
    "    return np.log10(least_squares_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interactive3DPlot:\n",
    "    def __init__(self, points: Dict[str, List[float]], ranges: Dict[str, Tuple[float, float]], width: int = 600, height: int = 600, draw_time: float = 0.1):\n",
    "        self.idle = True\n",
    "        self.points = points\n",
    "        self.draw_time = draw_time\n",
    "        self.threading = threading\n",
    "\n",
    "        self.range_weights = np.arange(  # Array with all possible weight values in the given range\n",
    "            ranges[\"x\"][0], ranges[\"x\"][1], 0.1\n",
    "        )\n",
    "        self.range_biases = np.arange(  # Array with all possible bias values in the given range\n",
    "            ranges[\"y\"][0], ranges[\"y\"][1], 0.1\n",
    "        )\n",
    "        self.range_biases_t = self.range_biases[:, np.newaxis]  # Bias array transposed\n",
    "        self.range_losses = []  # initialize z axis for 3D surface\n",
    "\n",
    "        self.ball = go.Scatter3d(  # initialize ball\n",
    "            x=[], y=[], z=[], hoverinfo=\"none\", mode=\"markers\", marker=dict(size=12, color=\"black\")\n",
    "        )\n",
    "\n",
    "        self.layout = go.Layout(\n",
    "            width=width,\n",
    "            height=height,\n",
    "            showlegend=False,\n",
    "            autosize=False,\n",
    "            margin=dict(t=0, l=0),\n",
    "            scene=dict(\n",
    "                xaxis=dict(title=\"Weight\", range=ranges[\"x\"], autorange=False, showticklabels=True),\n",
    "                yaxis=dict(title=\"Bias\", range=ranges[\"y\"], autorange=False, showticklabels=True),\n",
    "                zaxis=dict(title=\"Loss: log(MSE)\", range=ranges[\"z\"], autorange=True, showticklabels=False),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.data = [\n",
    "            go.Surface(\n",
    "                z=self.range_losses,\n",
    "                x=self.range_weights,\n",
    "                y=self.range_biases,\n",
    "                colorscale=\"Viridis\",\n",
    "                opacity=0.9,\n",
    "                showscale=False,\n",
    "                hoverinfo=\"none\",\n",
    "            ),\n",
    "            self.ball,\n",
    "        ]\n",
    "\n",
    "        self.plot = go.FigureWidget(self.data, self.layout)\n",
    "\n",
    "    def register_neuron(self, neuron: SimpleNeuron):\n",
    "        self.neuron = neuron\n",
    "        self.calc_surface()\n",
    "\n",
    "        # height of 3d surface represents loss of weight/bias combination\n",
    "        # In the 2D plot, x is an array from e.g. -4 to +4. But the weights and biases only have a single value\n",
    "        # Here x will be the points to do regression and to calculate the loss on. \n",
    "        # The surface is spanned by the arrays of weight and bias.\n",
    "        \n",
    "    def calc_surface(self):  \n",
    "                \n",
    "        self.neuron.weight = (  #instead of 1 weight and 1 bias, let Neuron have an array of all weights and biases\n",
    "            self.range_weights\n",
    "        )\n",
    "        self.neuron.bias = self.range_biases_t\n",
    "        self.range_losses = log_mse(  # result: matrix of losses of all weight/bias combinations in the given range\n",
    "            self.neuron, self.points\n",
    "        )\n",
    "        self.plot.data[0].z = self.range_losses\n",
    "\n",
    "    def update(self):\n",
    "        if self.idle:\n",
    "            thread = threading.Thread(target=self.redraw)\n",
    "            thread.start()\n",
    "\n",
    "    def redraw(self):  # when updating, only the ball is redrawn\n",
    "        self.idle = False\n",
    "        time.sleep(self.draw_time)\n",
    "        self.ball.x = [self.neuron.weight]\n",
    "        self.ball.y = [self.neuron.bias]\n",
    "        self.ball.z = [log_mse(self.neuron, self.points)]\n",
    "        self.plot.data[1].x = self.ball.x\n",
    "        self.plot.data[1].y = self.ball.y\n",
    "        self.plot.data[1].z = self.ball.z\n",
    "        self.idle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualPlot:\n",
    "    def __init__(self, points: Dict[str, List[float]], ranges_3d: Dict[str, Tuple[float, float]], ranges_2d: Dict[str, Tuple[float, float]]):\n",
    "        self.plot_3d = Interactive3DPlot(points, ranges_3d)\n",
    "        self.plot_2d = Interactive2DPlot(points, ranges_2d, width=400, height=500, margin=dict(t=200, l=30))\n",
    "\n",
    "    def register_neuron(self, neuron: SimpleNeuron):\n",
    "        self.plot_3d.register_neuron(neuron)\n",
    "        self.plot_2d.register_neuron(neuron)\n",
    "\n",
    "    def update(self):\n",
    "        self.plot_3d.update()\n",
    "        self.plot_2d.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Train the neuron\n",
    "<ul>\n",
    "<li> You are given the same set of 3 points and again one neuron to do a curve fit. Run the cell below.\n",
    "<li> <b>Change the weight and bias of the neuron using the sliders to minimize the loss.</b>\n",
    "<li> <b>Observe all changes.</b>\n",
    "    </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> You can turn the 3D-Plot by clicking on it and moving your cursor, but you have to stay inside the widget with your cursor. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed53a5e28d194a688ea1c38758a43eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=2.0, min=-2‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f266369651454c8acdd5fea3f79254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FigureWidget({\n",
       "    'data': [{'colorscale': [[0.0, '#440154'], [0.1111111111111111, '#482878'],\n",
       "‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ranges_3d = dict(x=(-2.5, 2.5), y=(-2.5, 2.5), z=(-1, 2.5))  # set up ranges for the 3d plot\n",
    "plot_task2 = DualPlot(points_linreg, ranges_3d, ranges_linreg)  # create a DualPlot object to mange plotting on two plots\n",
    "neuron_task2 = SimpleNeuron(plot_task2)  # create a new neuron for this task\n",
    "\n",
    "interact(\n",
    "    neuron_task2.set_values,\n",
    "    weight=FloatSlider(min=-2, max=2, step=0.2, layout=slider_layout),\n",
    "    bias=FloatSlider(min=-2, max=2, step=0.2, layout=slider_layout),\n",
    ")\n",
    "\n",
    "HBox((plot_task2.plot_3d.plot, plot_task2.plot_2d.plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> In general, what does the optimal weight and bias combination correspond to in the 3D Plot?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>its the point in the plot where the black ball is the lowest.(the black ball is belongs to the plot) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> What is the steepness of the valley at the point of optimal weight and bias combination?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>The steepness is -0.2 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1.4 Activation Functions\n",
    "By now our neuron model consisting of weights and biases is only capable of mimicing linear functions and all we can do is linear regression. Activation functions expand our capabilities by introducing an additional non-linearity into the neuron. With it, we can model more complex functions. The most commonly used activation function nowadays is the Rectified Linear Unit, also called **ReLU**. It just outputs the input value, as long as it's greater than 0. If it's lower than zero, it outputs 0. We can conveniently describe this function by taking the maximum of the input value and of 0. The greater value of both will be chosen as the output (5).\n",
    "\n",
    "\\begin{align}\n",
    "\\phi_{relu}(x) & = \\max(0,x)  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;    (5)\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define the ReLU function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input_val: np.ndarray) -> np.ndarray:\n",
    "    return np.maximum(input_val, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw a neuron with a ReLU activation function as follows:\n",
    "<img src=\"images/single_neuron_relu.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 5 - Neuron with ReLU activiation function visualized\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new class to implement this neuron in Python. We will inherit all properties of a neuron from SimpleNeuron.\n",
    "We only change the output by first feeding it through our ReLU function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b>  Implement a complete artificial neuron with relu activation function\n",
    "<ul>\n",
    "<li> Complete the code below for an artifical neuron by using the relu function from above to calculate its activation, like in Figure 5. </li>\n",
    "<li>Take a look at the <a href=\"#simple_neuron\">Simple Neuron Class</a> and write a similar compute function</li>\n",
    "<li>You don't need to re-implement the relu function and should not need to add more than 1 line. </li>\n",
    "\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluNeuron(SimpleNeuron): #inherit from SimpleNeuron class\n",
    "    def compute(self, x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        # STUDENT CODE HERE (1 pt)\n",
    "        self.activation = relu(np.dot(self.weight, x) + self.bias)\n",
    "        # STUDENT CODE until HERE\n",
    "        return self.activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 1.4.1 Task: Nonlinear Climate Control\n",
    "\n",
    "You find yourself as an engineer at the company \"ClimaTronics\". Your company wants to implement AI technology to regulate their new air conditioning system \"Perfect Climate 9000\". Even though the problem can be solved easily with conventional programming, the management department wants you to implement AI to attract investors. You have to fulfill the following requirements that are visualized in the datasheet excerpt:\n",
    "\n",
    "\n",
    "`The climate control shall remain off for temperatures under 25¬∞C. At a temperature of 30¬∞C, it shall reach 10% of its cooling power. Between 30¬∞C and 40¬∞C, the cooling power shall rise quadratically with the temperature. Cooling power shall reach its maximum at 40¬∞C.`\n",
    "<img src=\"images/datasheet.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below for to display a interactive plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bd5d49b8c3476e90b9fd0ae1c9500c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=10.0, min=-‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a82e0bf92fd447fbfe928104147eb33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '4f65079e-f588-4631-bd91-548033b942a7',\n",
       " ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "points_climate = dict(x=[25.0, 27.5, 30.0, 32.5, 35, 37.5, 40.0], y=[0.0, 2.0, 10.0, 23.7, 43, 68.7, 100.0])\n",
    "\n",
    "ranges_climate = dict(x=(-4, 45), y=(-4, 105))\n",
    "climate_plot = Interactive2DPlot(points_climate, ranges_climate)\n",
    "our_relu_neuron = ReluNeuron(climate_plot)\n",
    "\n",
    "interact(\n",
    "    our_relu_neuron.set_values,\n",
    "    weight=FloatSlider(min=-10, max=10, step=0.1, value=0, layout=slider_layout),\n",
    "    bias=FloatSlider(min=-200.0, max=200.0, step=1, value=0, layout=slider_layout),\n",
    ")\n",
    "\n",
    "climate_plot.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pt):</b> When setting the bias to 0.00, how does changing the weight affect the output function? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>without a bias the function is like y = ax with a being the weight, which result to a linear function passing through the origin</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> How does changing the bias affect the output function? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>with a bias the function has the for y = ax+b with b being the bias which results to changing the point where the function touches the x axis to the left or to the right</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b> When setting the weight to 1.00 and the bias to -10, at what temperature does the climate control start? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>It starts nearly at 10¬∞C</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b> When setting the weight to 1.00 and the bias to -20, at what temperature does the climate control start?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>It starts at 20¬∞C</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b> When setting the weight to 2.00 and the bias to -20, at what temperature does the climate control start?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>at 10¬∞C</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b> What's the best weight/bias configuration that you could find?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>The best configuration is : weight=7.10 and bias =-199</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Conclusion\n",
    "Using just one neuron, we can easily understand and retrace the influence of weight and bias.\n",
    "But our one-neuron-approximation is not enough to closely approximate the needed quadratic relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##  1.5 Neural Networks\n",
    "\n",
    "The approximation can be improved by using multiple neurons. Instead of just one neuron for our approximation, we construct a neural network. We will use two ReLU neurons and one output neuron that will have weights as well. Now we can decide how we want to weigh the result of the two ReLU neurons in the middle.\n",
    "\n",
    "### 1.5.1 Hidden Layers\n",
    "In this neural network, the two neurons in the middle represent a **hidden layer**.\n",
    "\n",
    "In the last task, the weight and bias had an easily traceable influence on the output.\n",
    "But by adding more neurons, the relationship between each weight and bias with the output becomes untraceable.\n",
    "We obtain the weights and biases by simply adapting them until the result turns out to be correct. In this process, we quickly loose overview of what exactly we are calculating. It becomes very hard to untangle a neuron and describe its responsibility in the system. \n",
    "\n",
    "The input value is multiplied by the first weights and after adding biases and running it through the activation function, the values are multiplied again by the second weights. Hidden layers can be stacked multiple times after one another. This gives room for multiple calculation steps, allowing more complex functions.\n",
    "\n",
    "Neural networks using at least one hidden layer have an interesting property: They can be used to approximate any continuous function. _(See \"Further Reading\")_\n",
    "\n",
    "<img src=\"images/hidden_layer.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a class for neural networks. The network will have four weights and two biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> For the sake of simplicity and code reusability, we will treat neural networks the same way we treat individual neurons in the past examples. Remember that an artificial neuron is only a mathematical function? A whole neural network can be also fully described by just one single function, as is done here when calculating the activation. The neurons don't have to take the concrete shape of individual data objects.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, plot: Interactive2DPlot):\n",
    "        self.plot = plot #I am assigned the following plot\n",
    "        self.plot.register_neuron(self) #hey plot, remember me\n",
    "        \n",
    "    def set_config(self, w_i1: float, w_o1: float, b1: float, w_i2: float, w_o2: float, b2: float):\n",
    "        self.w_i1 = w_i1\n",
    "        self.w_o1 = w_o1\n",
    "        self.b1 = b1\n",
    "        self.w_i2 = w_i2\n",
    "        self.w_o2 = w_o2\n",
    "        self.b2 = b2\n",
    "        self.show_config()\n",
    "        self.plot.update()  # please redraw my output\n",
    "\n",
    "    def show_config(self):\n",
    "        print(\"w_i1:\", self.w_i1, \"\\t| \", \"w_o1:\", self.w_o1,\"\\n\")\n",
    "        print(\"b1:\", self.b1, \"\\t| \", \"w_i2:\", self.w_i2,\"\\n\")\n",
    "        print(\"w_o2:\", self.w_o2, \"\\t| \", \"b2:\", self.b2,\"\\n\")\n",
    "\n",
    "    def compute(self, x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        self.prediction = (relu(self.w_i1 * x + self.b1) * self.w_o1\n",
    "                         + relu(self.w_i2 * x + self.b2) * self.w_o2)\n",
    "        return self.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "###  1.5.2 Task: Nonlinear Climate Control with Neural Network\n",
    "\n",
    "Run the cell below and adapt weights and bias to reach a better approximation of the desired curve than in the previous task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f811da09d94913a5932a60477ac9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w_i1', layout=Layout(width='90%'), max=10.0, min=-10‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4218f9642a0e49a6870b644a9f5308ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '32e2f832-9e10-429b-b7d3-3b4de3347a8d',\n",
       " ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "climate_plot_adv = Interactive2DPlot(points_climate, ranges_climate)\n",
    "our_neural_net = NeuralNetwork(climate_plot_adv)\n",
    "\n",
    "interact(\n",
    "    our_neural_net.set_config,\n",
    "    w_i1=FloatSlider(min=-10, max=10, step=0.1, layout=slider_layout),\n",
    "    w_o1=FloatSlider(min=-10, max=10, step=0.1,  layout=slider_layout),\n",
    "    b1=FloatSlider(min=-200.0, max=200.0, step=1,  layout=slider_layout),\n",
    "    w_i2=FloatSlider(min=-10, max=10, step=0.1, layout=slider_layout),\n",
    "    w_o2=FloatSlider(min=-10, max=10, step=0.1,  layout=slider_layout),\n",
    "    b2=FloatSlider(min=-200.0, max=200.0, step=1,layout=slider_layout),\n",
    ")\n",
    "climate_plot_adv.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b> What is the best configuration you could find? (Copy from above the plot)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>\n",
    "        \n",
    "w_i1: 0.6 \t|  w_o1: 6.0 \n",
    "\n",
    "b1: -16.0 \t|  w_i2: 0.7 \n",
    "\n",
    "w_o2: 9.9 \t|  b2: -23.0 \n",
    "\n",
    "Loss: 4.01\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 Conclusion\n",
    "We can conclude that the quadratic relationship can be better approximated by using additional weights and biases. Using two ReLU Neurons, we can create a function with two bends.\n",
    "However, the complexity of finding the optimal weights/biases increases drastically with each variable. The more powerful our neural networks should be, the harder the optimization becomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##  1.6 Backpropagation\n",
    "\n",
    "The solution to our optimization problems is called backpropagation. We can automate the process of adjusting weights and biases. In this example, we will turn back to the basics and use a simple neuron without an activation function. Backpropagation works by taking the partial derivatives of the loss function with respect to each weight and bias in the network. This can be done by using the chain rule of calculus. The network's output $\\hat{y} = \\hat{f}(x)$\n",
    "(if you denote $\\hat{y}$ as the predicted y-value by the neural network) is computed in the forward propagation by applying the given rules for calculation (multiply with weights, sum with bias and activation function until you reach the output). The loss is then calculated with your predicted and ground-truth value by the loss function. Given this loss you can easily calculate the partial derivatives in the so called backward propagation. See for example: [BackpropagationExample](https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html) \n",
    "\n",
    "At each point, the bias and weight gradients points to the direction of higher loss. The magnitude of the gradient represents the amount of increase in loss. \n",
    "\n",
    "Suppose we were to _maximize_ loss in Fig 6.: All we need to do is to follow the partial derivatives by adding them to our current weight/bias point. That means, decrease weight a lot (see the axes in Fig. 6), and decrease bias by some lesser amount, since it has less magnitude.\n",
    "\n",
    "However, because we want to go down, we will _subtract_ the gradient from out current point. This will move us closer to the minimum. In the next step, we are further down and next to the valley, but not close enough. So we just repeat the steps until we reach the minimum.\n",
    "\n",
    "The good thing about neural networks is that we can **analytically determine the gradient** for all possible data points. We do not have to estimate it by numerical methods, like calculating the loss of two weight/bias combinations and dividing it by the \"step distance\", as in the \"Euler method\". This beforehand knowledge of the gradient makes backpropagation relatively fast. However, sadly we can't analytically determine the weight/bias combination that brings the loss function to its minimum. We still have to apply it iteratively over many steps.\n",
    "\n",
    "Every step we take is called one **epoch**. (In this case _training steps_ and _epochs_ are equivalent). Because it is hard to determine whether the minimum is reached, we will specify the number of epochs before our descent and simply let the program run.\n",
    "\n",
    "If the magnitude of the gradients is too big, we will never reach a minimum. This is because our algorithm wants to move the ball too much at each step. It will oscillate around the minimum, but never arrive at it. In extreme cases, the movement even can oscillate up to infinty. To give us control over the amount of movement, the gradient is multiplied by a factor called **learning rate** (or also called \"step size\" in gradient descent). By setting it to an optimal value, we can prevent oscillations. However, if the learning rate is too small, the network will take forever to \"learn\", since the weights and biases are changing only very slowly.\n",
    "\n",
    "Number of epochs and learning rate are so called **hyperparameters**. They influence the training process but are not part of the network itself.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/backprop.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 6 - Partial derivatives of Loss function\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 Preparing Backpropagation Plot\n",
    "We will create a new 3D-Plot that tracks our past weight/bias/loss values as we try to optimize the loss step by step. The black ball will leave a trace of its past values. Run the cell below to enable plotting the backpropagation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_backprop = DualPlot(points_linreg, ranges_3d, ranges_linreg)\n",
    "trace_to_plot = go.Scatter3d(x=[], y=[], z=[], hoverinfo=\"none\", mode=\"lines\", line=dict(width=10, color=\"grey\"))\n",
    "\n",
    "plot_backprop.plot_3d.data.append(trace_to_plot)  # Expand 3D Plot to also plot traces\n",
    "plot_backprop.plot_3d.plot = go.FigureWidget(plot_backprop.plot_3d.data, plot_backprop.plot_3d.layout)\n",
    "plot_backprop.plot_3d.draw_time = 0\n",
    "\n",
    "\n",
    "def redraw_with_traces(plot_to_update: Interactive2DPlot, neuron: SimpleNeuron, trace_list: Dict[str, List[float]], points: Dict[str, List[float]]):  # executed every update step\n",
    "    plot_to_update.plot_3d.plot.data[2].x = trace_list[\"x\"]\n",
    "    plot_to_update.plot_3d.plot.data[2].y = trace_list[\"y\"]\n",
    "    plot_to_update.plot_3d.plot.data[2].z = trace_list[\"z\"]\n",
    "    plot_to_update.plot_3d.plot.data[1].x = [neuron.weight]\n",
    "    plot_to_update.plot_3d.plot.data[1].y = [neuron.bias]\n",
    "    plot_to_update.plot_3d.plot.data[1].z = [log_mse(neuron, points)]\n",
    "    plot_to_update.update()\n",
    "\n",
    "\n",
    "def add_traces(neuron: SimpleNeuron, points: Dict[str, List[float]], trace_list: Dict[str, List[float]]):  # executed every epoch\n",
    "    trace_list[\"x\"].extend([neuron.weight])\n",
    "    trace_list[\"y\"].extend([neuron.bias])\n",
    "    trace_list[\"z\"].extend([log_mse(neuron, points)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 1.6.2 DIY Backpropagation\n",
    "\n",
    "To do backpropagation, first you have to determine the partial derivatives of the loss function of the \"simple neuron\" with respect to weight and bias. After that, you have to figure out how to properly adjust the weights and biases to the gradient scaled to the learning rate.\n",
    "Down below at the end of the document you can verify your results by training. If you hit the benchmark, your algorithm is correct.\n",
    "\n",
    "The algorithm has to work with a dict of points of the form like: [points_linreg](#points_linreg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Determine the Gradient <b>analytically!!</b>\n",
    "<ul>\n",
    "<li> <b>Finish the function below by yourself.</b>\n",
    "<li> There are multiple solutions to this, your algorithm may adjust the weight and bias in the right direction despite the gradient calculation being wrong.\n",
    "<li> <b>Benchmark:</b> If you can reach a loss of 0.22 after 100 epochs and a learning rate of 0.03, your solution is correct\n",
    "    </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hint:</b>\n",
    "<ul>\n",
    "    <li> Read the above text on backpropagation carefully.\n",
    "    <li> If you are having trouble figuring the gradient out, try calculating the gradient on paper first.\n",
    "    <li> Ask yourself: What are the components of the Loss-function? How does the Loss-function depend on the weight and bias variables, by which you have to differentiate?\n",
    "    </li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_neuron_loss_gradient(neuron: SimpleNeuron, points: Dict[str, List[float]]) -> Dict[str, float]:\n",
    "\n",
    "    gradient_sum = dict(weight=0, bias=0) # contains the sum of the weight and bias gradient\n",
    "    for point_x, point_y in zip(points[\"x\"], points[\"y\"]):  # for each point\n",
    "            # Hint: point_x and point_y are the current point values\n",
    "\n",
    "        gradient_sum[\"weight\"] += ( # sum up the gradient for each point\n",
    "            \n",
    "            ### STUDENT CODE HERE (2 pts)\n",
    "            (2)*(point_x)*(neuron.weight*point_x+neuron.bias-point_y)\n",
    "            ### STUDENT CODE until HERE\n",
    "        )\n",
    "\n",
    "        gradient_sum[\"bias\"] += (\n",
    "            ### STUDENT CODE HERE (2 pts)\n",
    "            (2)*(neuron.weight*point_x+neuron.bias-point_y)\n",
    "            ### STUDENT CODE until HERE\n",
    "        )\n",
    "\n",
    "    gradient = dict(weight=gradient_sum[\"weight\"] / len(points[\"x\"]), bias=gradient_sum[\"bias\"] / len(points[\"x\"]))\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Adjust the Neuron\n",
    "<ul>\n",
    "\n",
    "<li> After finding the gradient you have to adjust the weight and bias of the neuron, based on the partial derivatives and the learning rate. You have to verify your results by training the net down below.\n",
    "<li> <b>Finish the function below by yourself.</b>\n",
    "    </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info:</b>\n",
    "<ul>\n",
    "    <li> This is an iterative function used on each neuron once per epoch.\n",
    "    <li> Use the neurons current weight and bias as a starting point and adjust it to improve the NN.\n",
    "    <li> The entered learning rate scales the magnitude of the adjustment.\n",
    "    <li> Think about the direction of the loss gradient and the direction you want your loss to shift in.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_neuron(neuron: SimpleNeuron, gradient: Dict[str, float], learning_rate: float):\n",
    "    ### STUDENT CODE HERE (2 pts)\n",
    "    neuron.weight = neuron.weight-learning_rate*gradient[\"weight\"]\n",
    "    neuron.bias = neuron.bias-learning_rate*gradient[\"bias\"]\n",
    "    ### STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3 Defining training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def train(neuron: SimpleNeuron, points: Dict[str, List[float]], epochs: int, learning_rate: float, redraw_step: int, trace_list: Dict[str, List[float]]):\n",
    "    redraw_with_traces(neuron.plot, neuron, trace_list, points)\n",
    "    for i in range(1, epochs + 1):  # first Epoch is Epoch no.1\n",
    "        add_traces(neuron, points, trace_list)\n",
    "        gradient = simple_neuron_loss_gradient(neuron, points)\n",
    "        adjust_neuron(neuron, gradient, learning_rate)\n",
    "\n",
    "        if i % redraw_step == 0:\n",
    "            print(\"Epoch:{} \\t\".format(i), end=\"\")\n",
    "            redraw_with_traces(neuron.plot, neuron_backprop, trace_list, points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Choose Hyperparameters and Train\n",
    "<ul>\n",
    "\n",
    "<li> Choose an optimal learning rate and number of epochs by trying out values and running the two cells below</li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b642b15b24c493d80ca56827c303342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FigureWidget({\n",
       "    'data': [{'colorscale': [[0.0, '#440154'], [0.1111111111111111, '#482878'],\n",
       "‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.145 #keep this for benchmarking, change to play around\n",
    "epochs = 100 # keep this for benchmarking, change to play around\n",
    "redraw_step = 10 # update plot every n'th epoch. too slow? set this to a higher value (e.g. 100)\n",
    "\n",
    "# these values are taken as parameters by the train function below\n",
    "\n",
    "neuron_backprop = SimpleNeuron(plot_backprop)\n",
    "HBox((plot_backprop.plot_3d.plot, plot_backprop.plot_2d.plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 18.45\n",
      "Loss: 18.45\n",
      "Epoch:10 \tLoss: 0.35\n",
      "Epoch:20 \tLoss: 0.22\n",
      "Epoch:30 \tLoss: 0.16\n",
      "Epoch:40 \tLoss: 0.12\n",
      "Epoch:50 \tLoss: 0.11\n",
      "Epoch:60 \tLoss: 0.10\n",
      "Epoch:70 \tLoss: 0.10\n",
      "Epoch:80 \tLoss: 0.10\n",
      "Epoch:90 \tLoss: 0.09\n",
      "Epoch:100 \tLoss: 0.09\n"
     ]
    }
   ],
   "source": [
    "#run this cell to test algorithm\n",
    "\n",
    "np.random.seed(4) # keep this for benchmarking, remove to play around\n",
    "\n",
    "neuron_backprop.set_values(  # set weight and bias randomly\n",
    "    (5 * np.random.random() - 2.5), (5 * np.random.random() - 2.5)\n",
    ")\n",
    "trace_list1 = dict(x=[], y=[], z=[])\n",
    "\n",
    "train(neuron_backprop, points_linreg, epochs, learning_rate, redraw_step, trace_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark:** If you can reach a loss of 0.22 after 100 epochs and a learning rate of 0.03, your solution is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only answer this after your algorithm has hit the benchmark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (3 pts):</b> What happens when you set the learing rate to 0.18? Explain this behavior.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>The Algorithm makes way to bigs steps so it misses the minimum each time and it repeat this again and again </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> What happens when you set the learing rate to 0.182? Explain this behavior.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>As the learning rate got bigger, the steps got bigger, but this time the loss got bigger too. The same happend for the next epoch which then put us in an kind of destructive resonance </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> What is the best learning rate you could find? (In terms of: lowest loss after 100 Epochs with lr=0.03) \n",
    "(Anything better than the benchmark loss of 0.22 is correct)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>0.145</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Machine/Deep Learning Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already introduced the learning rate, hyperparameters and epoch. Some further notation will be introduced now.\n",
    "\n",
    "Consider a training set you want to feed your neural network with and adjust its weights by using backpropagation. If you update weights and biases every sample that was used in one forward and backward pass, this is called **Stochastic Gradient Descent** or online learning. As you might already guess, you can as well accumulate errors by samples in the size of the so called **batch size** and perform multiple backward passes with these bigger subsets of training data, which is called **Batch Gradient Descent**. The proper calculation would be using the whole set of training samples for your forward passes and store errors to compute one update, which is the regular Gradient Descent. With increasing sample size this becomes often unfeasible and this is why batch gradient descent and stochastic gradient descent are helpful, but have different effects on the trained model during training.\n",
    "\n",
    "No matter what gradient descent variant you use, an **epoch** has expired if your training data has been totally used once for updating the weights in subsets or as a whole.\n",
    "\n",
    "As you already got to know in Task2 - PerformanceEvaluation, a model is able to overfit and underfit depending on the complexity of the problem/model and the amount of data available. Preventing Overfitting in neural networks is achieved by **Regularization**. In the Task7 - Convolutional Neural Networks you will have to apply some of these techniques by using the deeplearning library Keras. Most common techniques beside reducing model complexity or increasing/augmenting training data are **L1/L2-regularization** or **Dropout**.\n",
    "\n",
    "If you are not able to train your model properly this might be due to the fact that your model is confronted with **Exploding or Vanishing Gradients**. Consider a really deep model containing multiple hidden layers. By using the loss at the output the whole network's parameters should be updated. Because values $>1$ that are multiplied recursively will lead to larger updates, training behaviour can get instable. Training will not update weights in earlier layers when partial derivatives are $<1$ and are multiplied over many layers (Vanishing Gradient). To prevent the last case, which happens more often in regular training and by using sigmoid functions, the ReLU activation function is nowadays used as the default activation in intermediate layers. More advanced methods can be used as well, like **residual/skip connections**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Further Reading: Neural Networks are Universal Function Approximators\n",
    "\n",
    "It can be mathematically proven that neural networks can approximate any continuous function, as long as they have at least one hidden layer, use nonlinear activation functions, and use a sufficient (but finite) amount of hidden layer neurons. \n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/089360809190009T?via%3Dihub\n",
    "Kurt Hornik,\n",
    "Approximation capabilities of multilayer feedforward networks,\n",
    "Neural Networks,\n",
    "Volume 4, Issue 2,\n",
    "1991,\n",
    "Pages 251-257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "479.492px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
