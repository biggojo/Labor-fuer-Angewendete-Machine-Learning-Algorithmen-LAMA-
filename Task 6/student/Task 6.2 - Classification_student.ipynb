{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Classification with Neural Networks\n",
    "In this chapter, you will understand the workings of a classifier and manually train one that operates on a single value. You will improve the classifier step by step and learn fundamental concepts about classification as you go along.\n",
    "Finally, you will use automated backpropagation to train a multi-layer neural network to emulate a logic gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Introduction\n",
    "In machine learning and statistics, classification is the problem of identifying to which set of categories (sub-populations) a new observation belongs to, on the basis of a training set of data containing observations (or instances) whose category membership is known. Examples are assigning a given email to the \"spam\" or \"non-spam\" class or assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.). [1]\n",
    "\n",
    "A classification process requires a dataset that is split into different categories. A classifier can be trained on this dataset by learning the relationship between certain properties of the input data and the corresponding categories. \n",
    "To classify new data, the process is similar as in the chapter \"Regression\", however additional computational steps can be added depending on the application.\n",
    "A common classification problem that can be solved by neural networks is image recognition (seen in Figure 1).\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/neural_network_classification.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 1 - Image recognition by a neural network\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to import the necessary libraries and define a ReLU, MSE Loss function and a SimpleNeuron Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations # Used to allow referencing classes that have not yet been defined in type annotations. This will become default behaviour in Python 3.10. Until then, we have to use this line to enable that behaviour\n",
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "from ipywidgets import interact, Layout, FloatSlider\n",
    "import plotly.offline as plotly\n",
    "import plotly.graph_objs as go\n",
    "import time\n",
    "import threading\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input_val: np.ndarray) -> np.ndarray:\n",
    "    return np.maximum(input_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(predictions: np.ndarray, solutions: np.ndarray) -> float:\n",
    "    total_squared_loss = np.sum(np.subtract(predictions, solutions)**2) #np allows to handle both values and lists\n",
    "    mean_squared_loss = total_squared_loss/len(predictions)\n",
    "    return mean_squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuron:\n",
    "    def __init__(self, plot: Interactive2DPlot):\n",
    "        self.plot = plot #I am assigned the following plot\n",
    "        self.plot.register_neuron(self) #hey plot, remember me\n",
    "        \n",
    "    def set_values(self, weight: float, bias: float):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.plot.update() #hey plot, I have changed, redraw my output\n",
    "        \n",
    "    def get_weight(self) -> float:\n",
    "        return self.weight\n",
    "    \n",
    "    def get_bias(self) -> float:\n",
    "        return self.bias\n",
    "\n",
    "    def compute(self, x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        self.activation = np.dot(self.weight, x) + self.bias\n",
    "        return self.activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Interactive Plot monitors the activation of a neuron or a neural network\n",
    "class Interactive2DPlot:\n",
    "    def __init__(self, points_red: Dict[str, List[float]], points_blue: Dict[str, List[float]], ranges: Dict[str, Tuple[float, float]], loss_function: Callable[[np.ndarray, np.ndarray], float] = mean_squared_loss, loss_string: str = \"Loss\", width: int = 800, height: int = 400, margin: Dict[str, int] = { 't': 0, 'l': 170 }, draw_time: float = 0.1):\n",
    "        self.idle = True\n",
    "        self.points_red = points_red\n",
    "        self.points_blue = points_blue\n",
    "        self.draw_time = draw_time\n",
    "        self.loss_function = loss_function\n",
    "        self.loss_string = loss_string\n",
    "\n",
    "        self.x = np.arange(ranges[\"x\"][0], ranges[\"x\"][1], 0.01)\n",
    "        self.y = np.arange(ranges[\"y\"][0], ranges[\"y\"][1], 0.01)\n",
    "\n",
    "        self.layout = go.Layout(\n",
    "            xaxis=dict(title=\"Neck height in m\", range=ranges[\"x\"]),\n",
    "            yaxis=dict(title=\"y\", range=ranges[\"y\"]),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            showlegend=False,\n",
    "            margin=margin,\n",
    "        )\n",
    "        self.trace = go.Scatter(x=self.x, y=self.y)\n",
    "\n",
    "        self.plot_points_red = go.Scatter(\n",
    "            x=points_red[\"x\"], y=points_red[\"y\"], mode=\"markers\", marker=dict(color='rgb(255, 0, 0)', size=10)\n",
    "        )\n",
    "        self.plot_points_blue = go.Scatter(\n",
    "            x=points_blue[\"x\"],\n",
    "            y=points_blue[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color='rgb(0, 0, 255)', size=10, symbol=\"square\"),\n",
    "        )\n",
    "\n",
    "        self.plot_point_new = go.Scatter(\n",
    "            x=[], y=[], mode=\"markers\", marker=dict(size=20, symbol=\"star\", color='rgb(0,0,0)')\n",
    "        )\n",
    "\n",
    "        self.data = [self.trace, self.plot_points_red, self.plot_points_blue, self.plot_point_new]\n",
    "        self.plot = go.FigureWidget(self.data, self.layout)\n",
    "\n",
    "    def register_neuron(self, neuron: SimpleNeuron):\n",
    "        self.neuron = neuron\n",
    "\n",
    "    def redraw(self):\n",
    "        self.idle = False\n",
    "        time.sleep(self.draw_time)\n",
    "        self.plot.data[0].y = self.neuron.compute(self.x)\n",
    "        self.idle = True\n",
    "\n",
    "    def update(self):\n",
    "        loss_red = self.loss_function(self.neuron.compute(self.points_red[\"x\"]), self.points_red[\"y\"])\n",
    "        loss_blue = self.loss_function(self.neuron.compute(self.points_blue[\"x\"]), self.points_blue[\"y\"])\n",
    "        print(self.loss_string,\": {:0.3f}\".format((loss_red + loss_blue) / 2))\n",
    "\n",
    "        if self.idle:\n",
    "            thread = threading.Thread(target=self.redraw)\n",
    "            thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 From Regression to Classification\n",
    "\n",
    "###  2.2.1 Linear Regression\n",
    "\n",
    "You find yourself working on a farm with sheep and llamas grazing in seperate enclosures. However, last night the shepard forgot to close the gate between the two enclosures. The llamas and sheep now are mixed and have to be seperated again. You immediately come up with a machine learning based solution to separate the sheep from the llamas again: You assume that llamas can be distinguished from sheep by measuring the distance from the top of their head to their spine, since llamas have significantly longer necks. Using a LIDAR scanner, neck heights will be measured autonomously and the animals will be seperated using a food enticement and an electronic turnstile that only lets llamas through.\n",
    "\n",
    "\n",
    "<img src=\"images/neck_heights.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 2 - Concept of neck height measurement\n",
    "</p>\n",
    "\n",
    "To collect sample data, you go out on the field with a measuring tape and measure the neck heights of some sheep and llamas. You specify two categories: '0' for sheep and '1' for llamas. (See table 1)\n",
    "\n",
    "Most llamas are grown up and have long necks, but there are also some young llamas with smaller necks. However, since their necks are still longer than the sheeps', you figure that this won't be a problem.\n",
    "\n",
    "|  Animal | Neck height  | Category  |\n",
    "|---------|--------------|-----------|\n",
    "| Sheep #1| 0.20m        |0          |\n",
    "| Sheep #2| 0.23m        |0          |\n",
    "| Sheep #3| 0.28m        |0          |\n",
    "| Sheep #4| 0.32m        |0          |\n",
    "| Sheep #5| 0.35m        |0          |\n",
    "| Llama #1| 0.55m        |1          |\n",
    "| Llama #2| 0.68m        |1          |\n",
    "| Llama #3| 0.74m        |1          |\n",
    "| Llama #4| 0.83m        |1          |\n",
    "| Llama #5| 0.95m        |1          |\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    Table. 1 - Your data mining results\n",
    "</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.1 Training a Linear Regression Neuron by Hand\n",
    "For the sake of simplicity, you start by using a single neuron as a classifier. Run the two cells below to define the data mining points and to display a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_sheep = dict(\n",
    "              x=[ 0.20, 0.23, 0.28, 0.32, 0.35],\n",
    "              y=[ 0, 0, 0, 0, 0]\n",
    "             )\n",
    "\n",
    "points_llamas = dict(\n",
    "              x=[ 0.55, 0.68, 0.74, 0.83, 0.95],\n",
    "              y=[ 1,  1, 1, 1, 1]\n",
    "             )\n",
    "\n",
    "ranges = dict(x=[-0.1, 1.25], y=[-0.5, 1.4])\n",
    "slider_layout = Layout(width=\"90%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d4b6632d514eb9abf740b715caba86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=4.0, min=-2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daba02011fe34df087cd3c7503d1d5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '38580736-e810-48a9-91b6-6bb13b057066',\n",
       " …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot1 = Interactive2DPlot(points_sheep, points_llamas, ranges, loss_string=\"Mean Squared Loss\")\n",
    "neuron1 = SimpleNeuron(plot1)\n",
    "\n",
    "interact(\n",
    "    neuron1.set_values,\n",
    "    weight=FloatSlider(min=-2, max=4, step=0.1, layout = slider_layout),\n",
    "    bias=FloatSlider(min=-1, max=1, step=0.1, layout = slider_layout),\n",
    ")\n",
    "\n",
    "plot1.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1pt):</b> Change the weight and bias sliders above. What is a weight and bias combination that results in a loss < 0.05?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>weight=1.5    bias=-0.3</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### 2.2.1.2 Working our way towards a discrete classifier\n",
    "Now we want to use our trained neuron to classify new neck heights. To do that, we have to write a program that takes in a neck height and outputs what the trained neuron thinks about it. The classifier will also plot the new neck height. Run the box below to get the values from the task before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss : 0.500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549083a5124b4d3fb0d23984f98ba1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '4c263d48-2da2-4943-89b0-06f9c5862ed1',\n",
       " …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a duplicate of the last plot, so you don't have to scroll\n",
    "plot2 = Interactive2DPlot(points_sheep, points_llamas, ranges, loss_string=\"Mean Squared Loss\") \n",
    "neuron2 = SimpleNeuron(plot2)\n",
    "neuron2.set_values(neuron1.get_weight(), neuron1.get_bias()) #get your values from last task\n",
    "\n",
    "plot2.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Try to implement a classifier using just a linear neuron. (Yes, an almost futile task, but this will make sense later). <br> Complete the python code below and receive a classification_result.\n",
    "<ul>\n",
    "    <li> the classification result shall be the output of neuron2, given the new neck height </li>\n",
    "    <li> you shouldn't need to add more than 1 line of code </li>\n",
    "    <li> after executing, take a look at the star in the plot above. It represents the current input/output for the new neck length</li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 0.0\n"
     ]
    }
   ],
   "source": [
    "new_neck_height = 0.4  # this value shall be varied to answer the questions below\n",
    "\n",
    "classification_result: float\n",
    "\n",
    "### STUDENT CODE HERE (1pt)\n",
    "classification_result=neuron2.compute(new_neck_height)\n",
    "### STUDENT CODE until HERE\n",
    "\n",
    "plot2.plot.data[3].x = [new_neck_height] #update plot\n",
    "plot2.plot.data[3].y = [classification_result] \n",
    "\n",
    "print(\"Result:\", classification_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pt):</b> What classification value does the smallest llama have? (run the cell above and change new_neck_height)  \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>0.5250000000000001</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pt):</b> What classification value does an animal with a neck height of 0.1m have? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>0.14999999999999997</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pt):</b> What classification value does an animal with a neck height of 0.9m have?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>1.05</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> Why is the classification value continuous, even though the training data had only two discrete values? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>Because the neuron is returning its classification result without an activationfunktion we could use a step-funktion to get 1s and 0ros </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> How would you interpret this continuous classification value? Try to describe it in a few words, there is no single correct answer.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>the higher the classification_result the more likely it is to be a 1/lama the smaller the more likely to be a 0/sheep</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pt):</b> Your neuron outputs a continuous value, but what we need is a discrete output, that clearly says either \"llama\" or \"sheep\". To do this, you add a simple decision to the output of the neuron. The decision should be approximately just as sensitive towards llamas as to sheep. What neuron output (y-value) would you choose as the threshold and why? (no single correct answer)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>the smallest lama has a classification_result=0.5250000000000001 and the biggest sheep has a classification_result=0.22499999999999992 so 0,375 seems in the middle and somehow fine</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pt):</b> You want to add more data to your model to improve its performance. As you collect more data, you find a small llama with a neck height of 0.40m in your dataset. After you train your model on the new data, your discrete classifier decides that this small llama is a sheep. (Remember: the decision at the end only gets the y-value). Why is it problematic in this case to use a <b>linear</b> regression model for discrete classification? What property of the approximation function should be different?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>with a linear regression model the predicted value is continuous, not probabilistic and it is sensitive to imbalance data. The approximation funktion should not be linear</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pt):</b> You decide that that manually adding a discrete decision at the end of your network is an unpractical idea. It would be better to improve the linear neuron by adding a heaviside step function as an activation function, just like adding a ReLu function. Then the training could be automated and the right threshold could be found automatically. What is the problem with this approach if we still want to use the Backpropagation algorithm?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>A requirement for backpropagation is a differentiable activation function.The heaviside function is neither differentiable in the classical sense nor is it weakly differentiable.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.2.2 Logistic Regression\n",
    "\n",
    "In machine learning, the go-to assumption for an unknown two-class probability distribution is a logistic distribution.[2]\n",
    "Its cumulated function is the logistic function, of which the sigmoid function is the most used special case. (See Fig 3.)\n",
    "The sigmoid function enables a model to capture most natural occuring probability distributions.[3] (Further reading: see section \"Further Reading\" at the end of document)\n",
    "\n",
    "In the introduction of Task 2.1, we gave the neck lengths corresponding labels. \"0\" for sheep and \"1\" for llama.\n",
    "Here we can interpret the output of the neuron as the \"llama probability\": For example: An output of 1 means \"100%\" llama probability and an output of 0.2 means \"20%\" llama probability and so on.\n",
    "\n",
    "<img src=\"images/sigmoid.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 3 - Sigmoid function\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define a sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Complete Code and Train Neuron. Change the <code>SigmoidNeuron</code> class below to apply a sigmoid function to the final output.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidNeuron(SimpleNeuron): #inheriting from SimpleNeuron, \n",
    "                                   #all functions stay the same unless they are specified here\n",
    "\n",
    "    def compute(self, x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "        ### STUDENT CODE HERE (1 pt)\n",
    "        self.activation = sigmoid(np.dot(self.weight, x) + self.bias)\n",
    "        ### STUDENT CODE until HERE\n",
    "        return self.activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d2210f2fb64a848b0be5eb80cd6fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=200.0, min=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b754a32df3c4e64ad463f4a5ae924ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '8c666ed1-a35e-4dbc-8288-969a12337ca5',\n",
       " …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification_plot_sig = Interactive2DPlot(points_llamas, points_sheep, ranges, loss_string=\"Mean Squared Loss\")\n",
    "\n",
    "our_sig_neuron = SigmoidNeuron(classification_plot_sig)\n",
    "\n",
    "interact(\n",
    "    our_sig_neuron.set_values,\n",
    "    weight=FloatSlider(min=-50, max=200, step=0.1, layout = slider_layout),\n",
    "    bias=FloatSlider(min=-50, max=50, step=0.1, layout = slider_layout),\n",
    ")\n",
    "\n",
    "classification_plot_sig.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> Give one example of an optimal weight and bias combination.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>weight=30.2  bias=-13.5</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> What advantage does a classifier have in general that also outputs a probability compared to a classifier that just outputs a binary yes/no value? (a few words) \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>The user can assess the value of the classification</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> Give one example how we can use the additional probability information to increase the accuracy of our seperation process \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>If the probability is low we can implement another seperation step</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Cross Entropy/Logarithmic Loss:\n",
    "The most common loss function for classification is cross entropy loss, also called logarithmic loss. (In the context of machine learning, they are equal). In the special case of two categories, the loss is called binary cross entropy. The binary cross entropy loss between the ground truth data value $y$ and the predicted value $\\hat{y}$ is calculated as follows:\n",
    "\n",
    "\\begin{align}\n",
    "−[y \\cdot log(\\hat{y}) + (1 − y) \\cdot log(1 − \\hat{y})]\n",
    "\\end{align}\n",
    "\n",
    "In this manner, the average of all data points is calculated w.r.t. this loss.\n",
    "It turns out that the derivative of a logarithmic loss using one hot encoding (explained below) is just the solution vector subtracted by the network output, which makes it very easy to work with.\n",
    "**Note:** Cross entropy loss can only be used, if the output values are between 0 and 1.\n",
    "\n",
    "<img src=\"images/cross_entropy.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "        Fig. 4 - Logarithmic / cross entropy loss function\n",
    "\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (3 pts):</b> Calculate Squared and Cross Entropy Loss. Copy the table and fill out the ??? as an answer below (Markdown is fine to display the table). Use the cells below for calculations. \n",
    "\n",
    "| Input         | Llama Probability  |      Squared Loss    | Cross Entropy Loss   |\n",
    "|---------------|--------------------|----------------------|----------------------|\n",
    "|    llama(1)   | 0.99               |??????                |????                  |\n",
    "|    sheep(0)   | 0.6                |????                  |????                  |\n",
    "|    sheep(0)   | 0.95               |??????                |????                  |\n",
    "|    sheep(0)   | 0.999999           |????????              |?????                 |\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>\n",
    "\n",
    "| Input         | Llama Probability  |      Squared Loss    | Cross Entropy Loss   |\n",
    "|---------------|--------------------|----------------------|----------------------|\n",
    "|    llama(1)   | 0.99               |0.0001                |0.0101                |\n",
    "|    sheep(0)   | 0.6                |0.3600                |0.9163                |\n",
    "|    sheep(0)   | 0.95               |0.9025                |2.9957                |\n",
    "|    sheep(0)   | 0.999999           |1.0000                |13.8155               |\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(predictions: np.ndarray, solutions: np.ndarray) -> float:\n",
    "    predictions += 1e-15 #in order to prevent log(0)\n",
    "    total_loss = np.sum(-(solutions*np.log(predictions)+(1-solutions)*np.log(1-predictions)))\n",
    "    avg_loss = total_loss/len(predictions)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared loss: 1.0000\n",
      "cross entropy loss: 13.8155\n"
     ]
    }
   ],
   "source": [
    "predicted = np.array([0.999999]) #insert here\n",
    "actual = np.array([0]) #insert here\n",
    "\n",
    "\n",
    "print(\"mean squared loss: {:0.4f}\".format(mean_squared_loss(predicted,actual)))\n",
    "print(\"cross entropy loss: {:0.4f}\".format(cross_entropy_loss(predicted,actual)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> How do the goals of regression and classification generally differ? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>In classification we predict discrete categories or classes. In Regression we predict continuous quantities.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> Why do you think cross entropy loss is better suited for classification training algorithms?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>Because a drastically wrong classified subject has nearly the same mean squared loss than a medium wrong classified subject. The cross entropy loss is really different for the two.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 One-Hot Encoding\n",
    "To do classification, categories have to be represented in a way that the classifier can process. Neural networks cannot understand categories directly and need a numeric representation.\n",
    "\n",
    "### 2.4.1 Disadvantages of Integer Encoding\n",
    "\n",
    "In the llama classifier, llamas were assigned the value $1$ and sheep the value $0$. One single output neuron would \"fire\", if a llama was found, and not fire, if a sheep was found. This type of representing categories is called **integer** or **label encoding**\n",
    "\n",
    "This works reasonably well for binary classification, but what if we want to distinguish between sheep, llamas and shepherd dogs?\n",
    "Doing this with just one output neuron would result in complications: \n",
    "- Dogs would need a label that is numerically higher or lower (for example $2$), implying an order (Dogs > Llamas) where there actually is none.\n",
    "- it would be necessary to interpret three different states out of one output neuron value\n",
    "\n",
    "Another disadvantage can be seen in the next question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pt):</b> Suppose the encodings are: 0 for sheep, 1 for llamas and 2 for dogs. You classified 5 sheep and 5 dogs today. You want your classifier to output the average classification for today. What will the classifier say?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:the average is 1 -> lamas </b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Composition of One-Hot Encoding\n",
    "\n",
    "The solution for the shortcomings of integer encoding looks like this:\n",
    "\n",
    "| Input         | One Hot Encoding  | \n",
    "|---------------|--------------------|\n",
    "|    sheep   | [1,0,0]                |\n",
    "|    llama   | [0,1,0]               |\n",
    "|    dog     | [0,0,1]           |\n",
    "\n",
    "\n",
    "\n",
    "The length of the representation vector is always equal to the amount of categories. Only one element of the vector is 1 for each category (\"one-hot\").\n",
    "Using this encoding, we can conveniently use 3 output neurons for 3 different categories, so that the activation of each output neuron represents the classification score for that category.\n",
    "\n",
    "###  2.4.3 Limits of One-Hot Encoding\n",
    "One-hot encoding is not an unimprovable solution to represent categories, but rather another tool in the box that happens to work well for many problems, but not for all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pts):</b> Suppose you would like to train a speech recognition neural network that can classify all English words contained in the Oxford English Dictionary. It does not need to classify whole sentences, just single words. What would be a problem using one-hot encoding?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:there are 228132 words in the oxford english dictionary</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Softmax Activation Function\n",
    "\n",
    "The sigmoid function works fine for a \"yes or no\" problem, i.e. binary decisions. But more often than not we want to distinguish between more than two categories. For that, we need a function that takes in **multiple** neuron activations from the last layer of a network and outputs a **probability vector** containing the probabilities for each category. \n",
    "\n",
    "The key: **Each input** of this function is **normalized by the other inputs** such that the sum of the output vector is always 1. This activation function is different from ReLU or Sigmoid, because it always applies to the layer as a whole. In practice, it only makes sense as the activation function for the output layer.  Figure 3 shows an example network.\n",
    "\n",
    "We can realize a softmax activation function by taking each element $x_i$ of the input vector, calculating $\\exp(x_i)$ and then normalizing this value by dividing it by the sum of the $\\exp$ results of all single input vector elements. Strictly speaking, the $\\exp$ is not necessary for this effect - a linear normalization, limited to non-negative values, could also be interpreted as probability. However, the exponential normalization offers properties that improve performance (see \"further reading\").\n",
    "\n",
    "\\begin{align}\n",
    "(\\text{Softmax}(x))_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "<img src=\"images/softmax_example_network.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 3 - Softmax activation function\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pts):</b> In \"logistic regression\", we also obtained a probability by applying a sigmoid function on the last layers' output. Why can't we apply a sigmoid function on each output neuron of this network instead of a softmax and get a probability vector?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>if we apply the sigmoid function to each raw output value separately, this means our network can output that all of the classes have low probability, that one class has high probability but the other classes have low probability, or that multiple or all classes have high probability so if there is a dog and a lama in the picture sigmoids works fine. Because in softmax all values added have to be one there is only one correct answer.  </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2.6 Automated Classification Training\n",
    "\n",
    "### 2.6.1 Introduction\n",
    "\n",
    "We already have explored automated training using backpropagation in the last chapter. We had one set of points that we had to fit a function as close as possible. The task is similar for classification training. However instead of y-coordinates for points, we now have discrete categories.\n",
    "\n",
    "You got already a set of neck lengths and the correspoding categories (see table 1). In the field of machine learing, this dataset is called __training data__. It specifies the behaviour that the neural net should have. We will use backpropagation to adjust the weights and biases of the network over and over again until the network outputs the same values to a given set of inputs as in the training data. During backpropagation, the network is figuratively \"learning\" the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.6.2 Realizing an XOR Gate with a Neural Network\n",
    "\n",
    "You find yourself working as an engineer at a major electronic component manufacturing company. Your company wants to produce the first XOR gate chip that runs on artificial intelligence. You are given the training data in the form of a truth table:\n",
    "\n",
    "\n",
    "| Input 1| Input 2  | Output    |\n",
    "|--------|----------|-----------|\n",
    "|    0   | 0        |0          |\n",
    "|    0   | 1        |1          |\n",
    "|    1   | 0        |1          |\n",
    "|    1   | 1        |0          |\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    Table. 2 - XOR Truth table\n",
    "</p>\n",
    "\n",
    "\n",
    "In this task we will make use of arrays and matrices to ease the handling of the data and the network parameters. We will also utilize a neural network without biases in order to make the algorithm as simple as possible.\n",
    "The training data consists of a 2D Array of all possible input states and a 1D Array of all corresponding outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2.1 Task : Create Training Data\n",
    "\n",
    "A training set consists of an input set and a solution set. During supervised training, the network is adjusted until its predictions to the input set match the corresponding predetermined solutions.\n",
    "Complete the training data below using the truth table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Create Training Data. A training set consists of an input set and a solution set. During supervised training, the network is adjusted until its predictions to the input set match the corresponding predetermined solutions (not always see: Overfitting, but in this case). Complete the training data below using the groundtruth table above. Please initialize the solution '2 dimensional' as well.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_input_set: np.ndarray\n",
    "xor_solution_set: np.ndarray\n",
    "\n",
    "# STUDENT CODE HERE (1 pt)\n",
    "xor_input_set=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "xor_solution_set=np.array([[0],[1],[1],[0]])\n",
    "# STUDENT CODE until HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2.2 Initializing the Network\n",
    "Next, the Network has to be defined and initialized. For this task, we use a network with 3 hidden neurons (see Figure 4).\n",
    "\n",
    "<img src=\"images/3x2_xor_network.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 4 - Neural Network \n",
    "</p>\n",
    "\n",
    "We define $w_{01}, w_{02}, w_{03}, w_{10}, w_{11}, w_{12}$ all at once by just defining a 2x3 weight matrix $w_{l1}$ and do the same for $w_{l2}$. The matrices will be initialized with values between -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define a neural network class that is depicted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.hl_sum = [0, 0, 0]\n",
    "        self.hl_activation = [0, 0, 0]\n",
    "        self.ol_sum = [0]\n",
    "        self.prediction = 0\n",
    "        self.b = 0\n",
    "        self.w_i = np.zeros((2, 3))\n",
    "        self.w_o = np.zeros((3, 1))\n",
    "        \n",
    "    def set_conf(self, w_i: np.ndarray, w_o: np.ndarray, b: float):  # w_i and w_o are matrices here\n",
    "        self.w_i = w_i\n",
    "        self.w_o = w_o\n",
    "        self.b = b\n",
    "\n",
    "    def get_conf(self) -> Dict[str, Union[np.ndarray, float]]:\n",
    "        configuration = dict()\n",
    "        configuration['w_i'] = self.w_i\n",
    "        configuration['w_o'] = self.w_o\n",
    "        configuration['b'] = self.b\n",
    "        return configuration\n",
    "\n",
    "    def get_ex(self) -> Dict[str, float]:\n",
    "        excitations = dict();\n",
    "        excitations['hl_sum'] = self.hl_sum\n",
    "        excitations['hl_activation'] = self.hl_activation\n",
    "        excitations['ol_sum'] = self.ol_sum\n",
    "        return excitations\n",
    "    \n",
    "    \n",
    "    def show_conf(self):\n",
    "        print(\"weight matrix w_i:\")\n",
    "        print(self.w_i)\n",
    "        print(\"\\nweight matrix w_o:\")\n",
    "        print(self.w_o)\n",
    "        print(\"Bias\")\n",
    "        print(self.b)\n",
    "\n",
    "    def compute(self, input_set: np.ndarray) -> np.ndarray:\n",
    "        self.hl_sum = input_set.dot(self.w_i)\n",
    "        self.hl_activation = relu(self.hl_sum) \n",
    "        self.ol_sum = relu(self.hl_activation).dot(self.w_o) + self.b\n",
    "        self.prediction = sigmoid(self.ol_sum)\n",
    "\n",
    "        return self.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_gate_net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(net):\n",
    "    #np.random.seed(3)\n",
    "    weight_matrix_i = np.random.rand(2,3)  # a 2x3 matrix of weights\n",
    "    weight_matrix_o = np.random.rand(3,1)  # a 3x1 matrix of weights\n",
    "    bias = np.random.randn()\n",
    "    net.set_conf(weight_matrix_i,weight_matrix_o,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight matrix w_i:\n",
      "[[0.82110642 0.38274138 0.57762102]\n",
      " [0.97631337 0.94240176 0.56146912]]\n",
      "\n",
      "weight matrix w_o:\n",
      "[[0.54057056]\n",
      " [0.35252837]\n",
      " [0.51582657]]\n",
      "Bias\n",
      "1.2934261623179244\n"
     ]
    }
   ],
   "source": [
    "initialize_network(logic_gate_net) #just a test initialization to illustrate the weight matrices\n",
    "logic_gate_net.show_conf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2.3 Defining Training Process\n",
    "Finally, run the cells below to implement a backpropagation algorithm. Try to understand the code. See Fig. 4 for explanation of the variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x: np.ndarray) -> np.ndarray: #the derivative of sigmoid\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(net: NeuralNetwork, input_set: np.ndarray, solution_set: np.ndarray, learning_rate: float, epochs: int):\n",
    "    for t in range(epochs):\n",
    "        # Forward pass: compute predicted solution_set\n",
    "        predictions = net.compute(input_set)\n",
    "        # Compute and print loss\n",
    "        log_loss = cross_entropy_loss(predictions, solution_set)\n",
    "        \n",
    "        if (t % 5 == 0):  # only output every 5th epoch\n",
    "            print(\"Loss after Epoch {}: {:0.4f}\".format(t, log_loss))\n",
    "\n",
    "        #unravel variables here for readability\n",
    "        ol_sum = net.get_ex()['ol_sum']\n",
    "        hl_activation = net.get_ex()['hl_activation']\n",
    "        hl_sum = net.get_ex()['hl_sum']\n",
    "        w_i = net.get_conf()['w_i']\n",
    "        w_o = net.get_conf()['w_o']\n",
    "        b = net.get_conf()['b']\n",
    "        \n",
    "        # Backpropagation to compute gradients of w_i and w_o with respect to loss\n",
    "        # start from the loss at the end and then work towards the front\n",
    "        grad_ol_sum = sigmoid_prime(ol_sum) * (predictions - xor_solution_set)\n",
    "        grad_w_o = hl_activation.T.dot(grad_ol_sum)  # Gradient of Loss with respect to w_o\n",
    "        grad_hl_activation = grad_ol_sum.dot(w_o.T)  # the second layer's error\n",
    "        grad_hl_sum = hl_sum.copy()  # create a copy to work with\n",
    "        grad_hl_sum[hl_sum < 0] = 0  # the derivate of ReLU\n",
    "        grad_w_i = input_set.T.dot(grad_hl_sum * grad_hl_activation)  #\n",
    "\n",
    "        updated_weight_matrix_i = w_i - learning_rate * grad_w_i\n",
    "        updated_weight_matrix_o = w_o - learning_rate * grad_w_o\n",
    "        updated_bias = b - learning_rate * grad_ol_sum.sum()\n",
    "        net.set_conf(updated_weight_matrix_i, updated_weight_matrix_o,\n",
    "                       updated_bias)  # Apply updated weights to network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Choose Hyperparameters and Train\n",
    "<ul>\n",
    "<li> Choose an optimal learning rate and number of epochs by trying out values and running the cell below.\n",
    "<li> If your training data was correct, the network should be ready for use after training.\n",
    "A successfull training should result in a loss smaller than 0.02.\n",
    "                                                     \n",
    "<li><b>Hint:</b> Press Shift+Enter on the cell below and then the \"up\" arrow key to repeat the training easily.\n",
    "\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after Epoch 0: 0.7891\n",
      "Loss after Epoch 5: 0.6365\n",
      "Loss after Epoch 10: 0.6714\n",
      "Loss after Epoch 15: 0.0889\n",
      "Loss after Epoch 20: 0.0532\n",
      "Loss after Epoch 25: 0.0419\n",
      "Loss after Epoch 30: 0.0353\n",
      "Loss after Epoch 35: 0.0310\n",
      "Loss after Epoch 40: 0.0279\n",
      "Loss after Epoch 45: 0.0255\n",
      "Loss after Epoch 50: 0.0242\n",
      "Loss after Epoch 55: 0.0233\n",
      "Loss after Epoch 60: 0.0224\n",
      "Loss after Epoch 65: 0.0215\n",
      "Loss after Epoch 70: 0.0206\n",
      "Loss after Epoch 75: 0.0198\n",
      "Loss after Epoch 80: 0.0191\n",
      "Loss after Epoch 85: 0.0185\n",
      "Loss after Epoch 90: 0.0179\n",
      "Loss after Epoch 95: 0.0173\n"
     ]
    }
   ],
   "source": [
    "learning_rate: float\n",
    "epochs: int\n",
    "# STUDENT CODE HERE (2 pts)\n",
    "learning_rate=7\n",
    "epochs=99\n",
    "# STUDENT CODE until HERE\n",
    "\n",
    "initialize_network(logic_gate_net) #initialize again so you can just run this box and train a new network\n",
    "train(logic_gate_net, xor_input_set, xor_solution_set,learning_rate,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pt):</b> Why are the losses different each time you run the cell?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>Because the weights and the bias is initialized random</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pt):</b> What is a good learning rate that reaches a loss < 0.02 in < 100 epochs most of the time?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>learning rate = 7</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Classification Test. Run the cell below and change the sliders and do a validation check on your logic gate.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706fbf6091e64c778fc621d54b9f094e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='input1', layout=Layout(width='22%'), max=1.0, step=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def change(input1: float, input2: float):\n",
    "    input_vector = np.array([input1 * 1, input2 * 1])     # converting bool to float\n",
    "    prediction = logic_gate_net.compute(input_vector)\n",
    "    print(\"\\t input: {} \\t \\t output: {:0.9f}\".format(input_vector, prediction[0]))\n",
    "\n",
    "interact(\n",
    "    change,\n",
    "    input1=FloatSlider(min=0, max=1, step=1, layout=Layout(width=\"22%\")),\n",
    "    input2=FloatSlider(min=0, max=1, step=1, layout=Layout(width=\"22%\")),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Continuous Input Test. Change the sliders and observe the changes when the input is varied continuously instead of binary.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcac7d2d61641f39ce06b84afba0bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='input1', max=1.0), FloatSlider(value=0.0, descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(change, input1=0.0, input2=0.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> What can you observe when changing the sliders? How would you describe the general relationship between the two inputs and the output (a few words)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>If both inputs have the same value the output is minimal if they are far appart the output reaches its max</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (1 pt):</b>  Change the sliders to the training data values e.g.(1.00, 1.00). Does the output match the training data exactly? Why is that the case?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>It does not match the training values perfectly because the weights are chosen randomly so the neural network is sometimes learning that 1 and 1 is something different than 0 </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> The neural network now can do something more than just predicting the values of the input set that you gave it. What \"special ability\" has your network gained automatically? (<b>Hint:</b> Think about neural networks in general, the XOR gate is just an example)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>The special ability of the neural network is that its very flexible so it can be used for prettymuch every Problem</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b> How can this special ability be useful when applying neural networks to self-driving vehicles?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>The neural network is pretty flexible and in self-driving there are many different problems that the vehicle could face, even some the neural network hasnt learned before</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (2 pts):</b>Why does this ability make it easier to use a neural network for self-driving vehicles than traditional rule-based programming. (One pos. and neg. aspect)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>Because you can train the neural network with hours of driving data rather than having to think about every situation that possibly could appear. In the end you cant know for sure that the neural network will act as it should in every situation so it could have learned sth dangerous that just happens rarely and you will never know until it happens.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Task:</b> Create an OR Gate. Change the code above to train an OR Network and verfy your results with a test.\n",
    "\n",
    "</div>\n",
    "\n",
    "| Input 1| Input 2  | Output    |\n",
    "|--------|----------|-----------|\n",
    "|    0   | 0        |0          |\n",
    "|    0   | 1        |1          |\n",
    "|    1   | 0        |1          |\n",
    "|    1   | 1        |1          |\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    Table. 3 - OR Truth table\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Neural-Networks using DeepLearningLibraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[0, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]], dtype = 'float64')\n",
    "\n",
    "y_train = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [1]], dtype = 'float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1 Keras Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.9095 - accuracy: 0.2500\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.8615 - accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.8314 - accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.8086 - accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.7900 - accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.7742 - accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.7604 - accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.7481 - accuracy: 0.2500\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.7370 - accuracy: 0.2500\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.7269 - accuracy: 0.2500\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7175 - accuracy: 0.5000\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.7089 - accuracy: 0.5000\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7009 - accuracy: 0.5000\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6863 - accuracy: 0.7500\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.6796 - accuracy: 0.7500\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.6733 - accuracy: 0.7500\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.6673 - accuracy: 0.7500\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6616 - accuracy: 0.7500\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6561 - accuracy: 0.7500\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6509 - accuracy: 0.7500\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6459 - accuracy: 0.7500\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.6411 - accuracy: 0.7500\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6376 - accuracy: 0.7500\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6350 - accuracy: 0.7500\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.6325 - accuracy: 0.7500\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6301 - accuracy: 0.7500\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.6277 - accuracy: 0.7500\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6254 - accuracy: 0.7500\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6232 - accuracy: 0.7500\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.6209 - accuracy: 0.7500\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6187 - accuracy: 0.7500\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.6165 - accuracy: 0.7500\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6143 - accuracy: 0.7500\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.6121 - accuracy: 0.7500\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6098 - accuracy: 0.7500\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6074 - accuracy: 0.7500\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6049 - accuracy: 0.7500\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 969us/step - loss: 0.6022 - accuracy: 0.7500\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5994 - accuracy: 0.7500\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 993us/step - loss: 0.5965 - accuracy: 0.7500\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5933 - accuracy: 0.7500\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.5899 - accuracy: 0.7500\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.5863 - accuracy: 0.7500\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5825 - accuracy: 0.7500\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.5786 - accuracy: 0.7500\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5744 - accuracy: 0.7500\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.5701 - accuracy: 0.7500\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.5656 - accuracy: 0.7500\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 973us/step - loss: 0.5610 - accuracy: 0.7500\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.5564 - accuracy: 0.7500\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.5516 - accuracy: 0.7500\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.5468 - accuracy: 0.7500\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.5420 - accuracy: 0.7500\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.5371 - accuracy: 0.7500\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.5326 - accuracy: 0.7500\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.5285 - accuracy: 0.7500\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5244 - accuracy: 0.7500\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.5204 - accuracy: 0.7500\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.5165 - accuracy: 0.7500\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.5127 - accuracy: 0.7500\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5089 - accuracy: 0.7500\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 969us/step - loss: 0.5051 - accuracy: 0.7500\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5015 - accuracy: 0.7500\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4979 - accuracy: 0.7500\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4943 - accuracy: 0.7500\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.4909 - accuracy: 0.7500\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.4875 - accuracy: 0.7500\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.4841 - accuracy: 0.7500\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.4809 - accuracy: 0.7500\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4777 - accuracy: 0.7500\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4745 - accuracy: 0.7500\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.4714 - accuracy: 0.7500\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4683 - accuracy: 0.7500\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.4653 - accuracy: 0.7500\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.4623 - accuracy: 0.7500\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.4594 - accuracy: 0.7500\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4564 - accuracy: 0.7500\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.4535 - accuracy: 0.7500\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4506 - accuracy: 0.7500\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.4477 - accuracy: 0.7500\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.4447 - accuracy: 0.7500\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4418 - accuracy: 0.7500\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.4388 - accuracy: 0.7500\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.4358 - accuracy: 0.7500\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.4328 - accuracy: 0.7500\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4297 - accuracy: 0.7500\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.4267 - accuracy: 0.7500\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.4235 - accuracy: 0.7500\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.4204 - accuracy: 0.7500\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4172 - accuracy: 0.7500\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4140 - accuracy: 0.7500\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.4115 - accuracy: 0.7500\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.4080 - accuracy: 0.7500\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4048 - accuracy: 0.7500\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.4016 - accuracy: 0.7500\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3983 - accuracy: 0.7500\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3951 - accuracy: 0.7500\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.3923 - accuracy: 0.7500\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.3891 - accuracy: 0.7500\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.3858 - accuracy: 0.7500\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.3825 - accuracy: 0.7500\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3792 - accuracy: 0.7500\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.3764 - accuracy: 0.7500\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3732 - accuracy: 0.7500\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.7500\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3666 - accuracy: 0.7500\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3633 - accuracy: 0.7500\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 987us/step - loss: 0.3606 - accuracy: 0.7500\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3573 - accuracy: 0.7500\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3540 - accuracy: 0.7500\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3507 - accuracy: 0.7500\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3479 - accuracy: 0.7500\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.3448 - accuracy: 0.7500\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3415 - accuracy: 0.7500\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 993us/step - loss: 0.3382 - accuracy: 0.7500\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3355 - accuracy: 0.7500\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3324 - accuracy: 0.7500\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.3292 - accuracy: 0.7500\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.3259 - accuracy: 0.7500\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.3234 - accuracy: 0.7500\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3202 - accuracy: 0.7500\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3169 - accuracy: 0.7500\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 978us/step - loss: 0.3140 - accuracy: 0.7500\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.3113 - accuracy: 0.7500\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.3081 - accuracy: 0.7500\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.3049 - accuracy: 0.7500\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3026 - accuracy: 0.7500\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.2994 - accuracy: 0.7500\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2962 - accuracy: 0.7500\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.2939 - accuracy: 0.7500\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2908 - accuracy: 0.7500\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2876 - accuracy: 0.7500\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2853 - accuracy: 0.7500\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2823 - accuracy: 0.7500\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2791 - accuracy: 0.7500\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 969us/step - loss: 0.2770 - accuracy: 0.7500\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2739 - accuracy: 0.7500\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2710 - accuracy: 0.7500\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2688 - accuracy: 0.7500\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2657 - accuracy: 0.7500\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2631 - accuracy: 0.7500\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2606 - accuracy: 0.7500\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 970us/step - loss: 0.2575 - accuracy: 0.7500\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2555 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2525 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2500 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2476 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2446 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2428 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2398 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2376 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.2350 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 973us/step - loss: 0.2324 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.2303 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2275 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.2257 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.2227 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2210 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2181 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 973us/step - loss: 0.2163 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.2136 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2117 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 967us/step - loss: 0.2091 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2072 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2047 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2028 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2004 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.1986 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.1960 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.1944 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.1918 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.1903 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1877 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.1863 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1838 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1821 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.1800 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.1780 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.1763 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1739 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.1727 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1703 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.1688 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.1669 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.1648 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1635 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.1613 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.1598 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1581 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.1560 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.1549 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1529 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1510 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.1499 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1479 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.1463 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.1450 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1431 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1416 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b217041e88>"
      ]
     },
     "execution_count": 898,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Library and modules\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Init the model\n",
    "model = Sequential([\n",
    "    Dense(3, input_shape=(2,), activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Show optimizer\n",
    "rmsprop = optimizers.RMSprop(learning_rate=0.01, rho=0.9)\n",
    "\n",
    "# Compile\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])\n",
    "# Train\n",
    "model.fit(x_train, y_train, batch_size = 4,\n",
    "          epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Install pytorch on tf1 or a new environment by using the search tool (graphically) in anaconda navigator. Please do not change Cuda when installing differently! to run the code below.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2 PyTorch Example\n",
    "This example is just a reference for how the syntax will look when using PyTorch. You do not need to install PyTorch just to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop:\n",
      "Epoch:        0  |  Loss: 0.8618576526641846\n",
      "Epoch:       50  |  Loss: 0.007643511518836021\n",
      "Epoch:      100  |  Loss: 0.0003135695878881961\n",
      "Epoch:      150  |  Loss: 3.922062387573533e-05\n",
      "Epoch:      200  |  Loss: 7.987054232216906e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3, True)\n",
    "        self.fc2 = nn.Linear(3, 1, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "inputs = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "targets = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training loop:\")\n",
    "for idx in range(0, 201):\n",
    "    for input, target in zip(inputs, targets):\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = net(input)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "    if idx % 50 == 0:\n",
    "        print(\"Epoch: {: >8}  |  Loss: {}\".format(idx, loss.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Outlook: Classification Tests in the Real World\n",
    "\n",
    "A classic application of neural networks is the classification of images. A commonly used data set is CIFAR-10, which consists of:  \n",
    " 1. Images of  airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks  (10 Categories)\n",
    " 2. Labels attached to each image that categorize the image\n",
    " \n",
    "<img src=\"images/cifar10_plot.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 3 - CIFAR-10 dataset[4]\n",
    "</p>\n",
    "\n",
    " \n",
    "The labels (also called annotations) act as the \"solution\" for the training set. Each item (airplane, car..) is a separate category. \n",
    "During training, the weights and biases in the network are adjusted in just the right way, until it performs the right mathematical operations to correctly classify the given training data. After training, the network can recognize whether the image is a cat, an airplane, etc. This even works for pictures that the network has never seen. You will find out how neural networks can perform image classification in the next class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "[1] Wikipedia, Statistical classification https://en.wikipedia.org/wiki/Statistical_classification, retrieved 01.05.2019\n",
    "\n",
    "[2]  Brownlee, Jason 2018. Machine Learning Algorithms From Scratch. p. 70\n",
    "\n",
    "[3]  Gibbs, M.N. (Nov 2000). \"Variational Gaussian process classifiers\". IEEE Transactions on Neural Networks. p. 1458–1464.\n",
    "\n",
    "[4] Cifar-10, Cifar-100 Dataset Introduction\n",
    "Corochann - https://corochann.com/cifar-10-cifar-100-dataset-introduction-1258.html, retrieved 02.02.2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "The Sigmoid Function in Logistic Regression: http://karlrosaen.com/ml/notebooks/logistic-regression-why-sigmoid/\n",
    "\n",
    "Why Softmax uses exponential function: https://stackoverflow.com/questions/17187507/why-use-softmax-as-opposed-to-standard-normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback and Recap\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Question (3pt):</b>  Please conclude in a few sentences what you learned in this exercise\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Answer:</b>In 6.2 we used neural networks to classify data and learned the bennefits and losses of these.In the end we got a short look on how to use the nn with deeplearning libraries.</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "395.996px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
